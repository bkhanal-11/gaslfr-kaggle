{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-30T11:13:48.859731Z","iopub.status.busy":"2023-06-30T11:13:48.859221Z","iopub.status.idle":"2023-06-30T11:13:58.801065Z","shell.execute_reply":"2023-06-30T11:13:58.800067Z","shell.execute_reply.started":"2023-06-30T11:13:48.859697Z"},"trusted":true},"outputs":[],"source":["from tensorflow.keras import layers\n","from tensorflow import keras\n","import tensorflow as tf\n","\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","import pandas as pd\n","import numpy as np\n","import json\n","import os\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-30T11:14:31.712897Z","iopub.status.busy":"2023-06-30T11:14:31.712088Z","iopub.status.idle":"2023-06-30T11:14:32.077107Z","shell.execute_reply":"2023-06-30T11:14:32.076070Z","shell.execute_reply.started":"2023-06-30T11:14:31.712860Z"},"trusted":true},"outputs":[],"source":["gpus = tf.config.experimental.list_physical_devices('GPU')\n","for gpu in gpus:\n","    print(\"Name:\", gpu.name, \"  Type:\", gpu.device_type)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-30T11:14:45.408122Z","iopub.status.busy":"2023-06-30T11:14:45.407717Z","iopub.status.idle":"2023-06-30T11:14:45.413148Z","shell.execute_reply":"2023-06-30T11:14:45.412104Z","shell.execute_reply.started":"2023-06-30T11:14:45.408089Z"},"trusted":true},"outputs":[],"source":["gpus = tf.config.experimental.list_physical_devices('GPU')\n","\n","if gpus:\n","  try:\n","    tf.config.experimental.set_visible_devices(gpus, 'GPU')\n","  except RuntimeError as e:\n","    print(e)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-30T11:14:46.432337Z","iopub.status.busy":"2023-06-30T11:14:46.431653Z","iopub.status.idle":"2023-06-30T11:14:46.438412Z","shell.execute_reply":"2023-06-30T11:14:46.437115Z","shell.execute_reply.started":"2023-06-30T11:14:46.432303Z"},"trusted":true},"outputs":[],"source":["class CONFIG:\n","    ROOT_DIR = '/kaggle/input/asl-fingerspelling'\n","    TRAIN_DATA = '/kaggle/input/asl-fingerspelling/train.csv'\n","    TRAIN_DIR = '/kaggle/input/asl-fingerspelling/train_landmarks'\n","    CHAR_PREDICTION_INDEX_MAP = '/kaggle/input/asl-fingerspelling/character_to_prediction_index.json'\n","    SUP_METADATA = '/kaggle/input/asl-fingerspelling/supplemental_metadata.csv'\n","    SUP_LANDMARK_DIR = '/kaggle/input/asl-fingerspelling/supplemental_landmarks'\n","    \n","    OUTPUT_PREPROCESSING = '/kaggle/input/gasl-fingerspelling-preprocessed'\n","\n","    start_token = '<'\n","    end_token = '>'\n","    mask_token = '^'\n","    mask_token_idx = 59\n","    start_token_idx = 60\n","    end_token_idx = 61"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-30T11:14:47.225952Z","iopub.status.busy":"2023-06-30T11:14:47.224279Z","iopub.status.idle":"2023-06-30T11:14:47.237479Z","shell.execute_reply":"2023-06-30T11:14:47.236407Z","shell.execute_reply.started":"2023-06-30T11:14:47.225910Z"},"trusted":true},"outputs":[],"source":["with open (CONFIG.CHAR_PREDICTION_INDEX_MAP, \"r\") as f:\n","    char_to_ord = json.load(f)\n","\n","char_to_ord[CONFIG.mask_token] = CONFIG.mask_token_idx\n","char_to_ord[CONFIG.start_token] = CONFIG.start_token_idx\n","char_to_ord[CONFIG.end_token] = CONFIG.end_token_idx\n","\n","ord_to_char = {j:i for i, j in char_to_ord.items()}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-30T11:14:48.305778Z","iopub.status.busy":"2023-06-30T11:14:48.305343Z","iopub.status.idle":"2023-06-30T11:14:48.320702Z","shell.execute_reply":"2023-06-30T11:14:48.318760Z","shell.execute_reply.started":"2023-06-30T11:14:48.305743Z"},"trusted":true},"outputs":[],"source":["# Memory saving function credit to https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n","def reduce_mem_usage(df):\n","    \"\"\" iterate through all the columns of a dataframe and modify the data type\n","        to reduce memory usage.        \n","    \"\"\"\n","    #start_mem = df.memory_usage().sum() / 1024**2\n","    #print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n","\n","    for col in df.columns:\n","        col_type = df[col].dtype\n","\n","        if col_type != object:\n","            c_min = df[col].min()\n","            c_max = df[col].max()\n","            if str(col_type)[:3] == 'int':\n","                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n","                    df[col] = df[col].astype(np.int8)\n","                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n","                    df[col] = df[col].astype(np.int16)\n","                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n","                    df[col] = df[col].astype(np.int32)\n","                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n","                    df[col] = df[col].astype(np.int64)  \n","            else:\n","                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n","                    df[col] = df[col].astype(np.float16)\n","                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n","                    df[col] = df[col].astype(np.float32)\n","                else:\n","                    df[col] = df[col].astype(np.float64)\n","\n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-30T11:14:49.105774Z","iopub.status.busy":"2023-06-30T11:14:49.105376Z","iopub.status.idle":"2023-06-30T11:14:49.302714Z","shell.execute_reply":"2023-06-30T11:14:49.301614Z","shell.execute_reply.started":"2023-06-30T11:14:49.105743Z"},"trusted":true},"outputs":[],"source":["train_df = pd.read_csv(CONFIG.TRAIN_DATA)\n","train_df[\"phrase_bytes\"] = train_df[\"phrase\"].map(lambda x: x.encode(\"utf-8\"))\n","train_df = reduce_mem_usage(train_df)\n","train_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-30T11:14:52.340021Z","iopub.status.busy":"2023-06-30T11:14:52.339414Z","iopub.status.idle":"2023-06-30T11:14:52.346405Z","shell.execute_reply":"2023-06-30T11:14:52.345237Z","shell.execute_reply.started":"2023-06-30T11:14:52.339980Z"},"trusted":true},"outputs":[],"source":["# Can be added more landmark here\n","LPOSE = [13, 15, 17, 19, 21]\n","RPOSE = [14, 16, 18, 20, 22]\n","POSE = LPOSE + RPOSE\n","# POSE = list(range(0, 32))\n","\n","FACE = [0, 9, 11, 13, 14, 17, 117, 118, 119, 199, 346, 347, 348]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-30T11:14:53.307621Z","iopub.status.busy":"2023-06-30T11:14:53.306614Z","iopub.status.idle":"2023-06-30T11:14:53.315980Z","shell.execute_reply":"2023-06-30T11:14:53.314710Z","shell.execute_reply.started":"2023-06-30T11:14:53.307585Z"},"trusted":true},"outputs":[],"source":["X = [f'x_right_hand_{i}' for i in range(21)] + [f'x_left_hand_{i}' for i in range(21)] + [f'x_pose_{i}' for i in POSE] + [f'x_face_{i}' for i in FACE]\n","Y = [f'y_right_hand_{i}' for i in range(21)] + [f'y_left_hand_{i}' for i in range(21)] + [f'y_pose_{i}' for i in POSE] + [f'y_face_{i}' for i in FACE]\n","Z = [f'z_right_hand_{i}' for i in range(21)] + [f'z_left_hand_{i}' for i in range(21)] + [f'z_pose_{i}' for i in POSE] + [f'z_face_{i}' for i in FACE]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-30T11:14:53.684181Z","iopub.status.busy":"2023-06-30T11:14:53.683839Z","iopub.status.idle":"2023-06-30T11:14:53.693971Z","shell.execute_reply":"2023-06-30T11:14:53.692836Z","shell.execute_reply.started":"2023-06-30T11:14:53.684144Z"},"trusted":true},"outputs":[],"source":["SEL_COLS = X + Y + Z\n","FRAME_LEN = 128\n","\n","X_IDX = [i for i, col in enumerate(SEL_COLS)  if \"x_\" in col]\n","Y_IDX = [i for i, col in enumerate(SEL_COLS)  if \"y_\" in col]\n","Z_IDX = [i for i, col in enumerate(SEL_COLS)  if \"z_\" in col]\n","\n","RHAND_IDX = [i for i, col in enumerate(SEL_COLS)  if \"right\" in col]\n","LHAND_IDX = [i for i, col in enumerate(SEL_COLS)  if \"left\" in col]\n","RPOSE_IDX = [i for i, col in enumerate(SEL_COLS)  if \"pose\" in col and int(col[-2:]) in RPOSE]\n","LPOSE_IDX = [i for i, col in enumerate(SEL_COLS)  if \"pose\" in col and int(col[-2:]) in LPOSE]\n","FACE_IDX  = [i for i, col in enumerate(SEL_COLS)  if \"face\" in col]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-30T11:14:55.523478Z","iopub.status.busy":"2023-06-30T11:14:55.523110Z","iopub.status.idle":"2023-06-30T11:14:55.549982Z","shell.execute_reply":"2023-06-30T11:14:55.548956Z","shell.execute_reply.started":"2023-06-30T11:14:55.523448Z"},"trusted":true},"outputs":[],"source":["# Preprocessing the landmarks\n","def resize_pad(x):\n","    if tf.shape(x)[0] < FRAME_LEN:\n","        x = tf.pad(x, ([[0, FRAME_LEN-tf.shape(x)[0]], [0, 0], [0, 0]]))\n","    else:\n","        x = tf.image.resize(x, (FRAME_LEN, tf.shape(x)[1]))\n","    return x\n","\n","\n","def preprocess_landmark(x):\n","    rhand = tf.gather(x, RHAND_IDX, axis=1)\n","    lhand = tf.gather(x, LHAND_IDX, axis=1)\n","    rpose = tf.gather(x, RPOSE_IDX, axis=1)\n","    lpose = tf.gather(x, LPOSE_IDX, axis=1)\n","    face  = tf.gather(x, FACE_IDX, axis=1)\n","    \n","    rnan_idx = tf.reduce_any(tf.math.is_nan(rhand), axis=1)\n","    lnan_idx = tf.reduce_any(tf.math.is_nan(lhand), axis=1)\n","    \n","    rnans = tf.math.count_nonzero(rnan_idx)\n","    lnans = tf.math.count_nonzero(lnan_idx)\n","    \n","    # For dominant hand\n","    if rnans > lnans:\n","        hand = lhand\n","        pose = lpose\n","        face = face\n","        \n","        hand_x = hand[:, 0*(len(LHAND_IDX)//3) : 1*(len(LHAND_IDX)//3)]\n","        hand_y = hand[:, 1*(len(LHAND_IDX)//3) : 2*(len(LHAND_IDX)//3)]\n","        hand_z = hand[:, 2*(len(LHAND_IDX)//3) : 3*(len(LHAND_IDX)//3)]\n","        hand = tf.concat([1-hand_x, hand_y, hand_z], axis=1)\n","        \n","        pose_x = pose[:, 0*(len(LPOSE_IDX)//3) : 1*(len(LPOSE_IDX)//3)]\n","        pose_y = pose[:, 1*(len(LPOSE_IDX)//3) : 2*(len(LPOSE_IDX)//3)]\n","        pose_z = pose[:, 2*(len(LPOSE_IDX)//3) : 3*(len(LPOSE_IDX)//3)]\n","        pose = tf.concat([1-pose_x, pose_y, pose_z], axis=1)\n","    else:\n","        hand = rhand\n","        pose = rpose\n","        face = face\n","    \n","    hand_x = hand[:, 0*(len(LHAND_IDX)//3) : 1*(len(LHAND_IDX)//3)]\n","    hand_y = hand[:, 1*(len(LHAND_IDX)//3) : 2*(len(LHAND_IDX)//3)]\n","    hand_z = hand[:, 2*(len(LHAND_IDX)//3) : 3*(len(LHAND_IDX)//3)]\n","    hand = tf.concat([hand_x[..., tf.newaxis], hand_y[..., tf.newaxis], hand_z[..., tf.newaxis]], axis=-1)\n","    \n","    mean = tf.math.reduce_mean(hand, axis=1)[:, tf.newaxis, :]\n","    std = tf.math.reduce_std(hand, axis=1)[:, tf.newaxis, :]\n","    hand = (hand - mean) / std\n","\n","    pose_x = pose[:, 0*(len(LPOSE_IDX)//3) : 1*(len(LPOSE_IDX)//3)]\n","    pose_y = pose[:, 1*(len(LPOSE_IDX)//3) : 2*(len(LPOSE_IDX)//3)]\n","    pose_z = pose[:, 2*(len(LPOSE_IDX)//3) : 3*(len(LPOSE_IDX)//3)]\n","    pose = tf.concat([pose_x[..., tf.newaxis], pose_y[..., tf.newaxis], pose_z[..., tf.newaxis]], axis=-1)\n","    \n","    mean = tf.math.reduce_mean(pose, axis=1)[:, tf.newaxis, :]\n","    std = tf.math.reduce_std(pose, axis=1)[:, tf.newaxis, :]\n","    pose = (pose - mean) / std\n","    \n","    face_x = face[:, 0*(len(FACE_IDX)//3) : 1*(len(FACE_IDX)//3)]\n","    face_y = face[:, 1*(len(FACE_IDX)//3) : 2*(len(FACE_IDX)//3)]\n","    face_z = face[:, 2*(len(FACE_IDX)//3) : 3*(len(FACE_IDX)//3)]\n","    face = tf.concat([face_x[..., tf.newaxis], face_y[..., tf.newaxis], face_z[..., tf.newaxis]], axis=-1)\n","    \n","    mean = tf.math.reduce_mean(face, axis=1)[:, tf.newaxis, :]\n","    std = tf.math.reduce_std(face, axis=1)[:, tf.newaxis, :]\n","    face = (face - mean) / std\n","    \n","    x = tf.concat([hand, pose, face], axis=1)\n","    x = resize_pad(x)\n","    \n","    x = tf.where(tf.math.is_nan(x), tf.zeros_like(x), x)\n","    x = tf.reshape(x, (FRAME_LEN, len(LHAND_IDX) + len(LPOSE_IDX) + len(FACE_IDX)))\n","    return x"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-30T11:14:57.000879Z","iopub.status.busy":"2023-06-30T11:14:57.000486Z","iopub.status.idle":"2023-06-30T11:14:57.006298Z","shell.execute_reply":"2023-06-30T11:14:57.004990Z","shell.execute_reply.started":"2023-06-30T11:14:57.000845Z"},"trusted":true},"outputs":[],"source":["# def load_relevant_data_subset(pq_path):\n","#     return pd.read_parquet(pq_path)\n","\n","def load_relevant_data_subset(pq_path):\n","    return pd.read_parquet(pq_path, columns=SEL_COLS)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-30T11:14:57.484408Z","iopub.status.busy":"2023-06-30T11:14:57.483997Z","iopub.status.idle":"2023-06-30T11:15:03.640460Z","shell.execute_reply":"2023-06-30T11:15:03.639429Z","shell.execute_reply.started":"2023-06-30T11:14:57.484375Z"},"trusted":true},"outputs":[],"source":["file_id = train_df.file_id.iloc[0]\n","pqfile = f\"{CONFIG.ROOT_DIR}/train_landmarks/{file_id}.parquet\"\n","seq_refs = train_df.loc[train_df.file_id == file_id]\n","seqs = load_relevant_data_subset(pqfile)\n","\n","seq_id = seq_refs.sequence_id.iloc[0]\n","frames = seqs.iloc[seqs.index == seq_id]\n","phrase = str(train_df.loc[train_df.sequence_id == seq_id].phrase.iloc[0])\n","\n","print(preprocess_landmark(frames).shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-30T11:17:23.892321Z","iopub.status.busy":"2023-06-30T11:17:23.891635Z","iopub.status.idle":"2023-06-30T11:17:23.919740Z","shell.execute_reply":"2023-06-30T11:17:23.918583Z","shell.execute_reply.started":"2023-06-30T11:17:23.892273Z"},"trusted":true},"outputs":[],"source":["table = tf.lookup.StaticHashTable(\n","    initializer=tf.lookup.KeyValueTensorInitializer(\n","        keys=list(char_to_ord.keys()),\n","        values=list(char_to_ord.values()),\n","    ),\n","    default_value=tf.constant(-1),\n","    name=\"class_weight\"\n",")\n","\n","def preprocess_fn(landmarks, phrase):\n","    phrase = CONFIG.start_token + phrase + CONFIG.end_token\n","    phrase = tf.strings.bytes_split(phrase)\n","    phrase = table.lookup(phrase)\n","    phrase = tf.pad(phrase, paddings=[[0, 64 - tf.shape(phrase)[0]]], mode='CONSTANT', constant_values=CONFIG.mask_token_idx)\n","    \n","    return preprocess_landmark(landmarks), phrase\n","\n","def decode_fn(record_bytes):\n","    schema = {COL: tf.io.VarLenFeature(dtype=tf.float32) for COL in SEL_COLS}\n","    schema[\"phrase\"] = tf.io.FixedLenFeature([], dtype=tf.string)\n","    features = tf.io.parse_single_example(record_bytes, schema)\n","    phrase = features[\"phrase\"]\n","    landmarks = ([tf.sparse.to_dense(features[COL]) for COL in SEL_COLS])\n","    landmarks = tf.transpose(landmarks)\n","    return landmarks, phrase"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-30T11:17:28.609859Z","iopub.status.busy":"2023-06-30T11:17:28.609468Z","iopub.status.idle":"2023-06-30T11:17:31.040976Z","shell.execute_reply":"2023-06-30T11:17:31.039950Z","shell.execute_reply.started":"2023-06-30T11:17:28.609827Z"},"trusted":true},"outputs":[],"source":["tffiles = train_df.file_id.map(lambda x: f'{CONFIG.OUTPUT_PREPROCESSING}/{x}.tfrecord').unique()\n","\n","batch_size = 32\n","val_len = int(0.05 * len(tffiles))\n","\n","train_dataset = tf.data.TFRecordDataset(tffiles[val_len:]).map(decode_fn).map(preprocess_fn).shuffle(30000, reshuffle_each_iteration=True).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n","val_dataset = tf.data.TFRecordDataset(tffiles[:val_len]).map(decode_fn).map(preprocess_fn).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-30T11:17:35.417195Z","iopub.status.busy":"2023-06-30T11:17:35.416702Z","iopub.status.idle":"2023-06-30T11:17:35.442257Z","shell.execute_reply":"2023-06-30T11:17:35.441123Z","shell.execute_reply.started":"2023-06-30T11:17:35.417146Z"},"trusted":true},"outputs":[],"source":["class TokenEmbedding(layers.Layer):\n","    def __init__(self, num_vocab=1000, maxlen=100, num_hid=64):\n","        super().__init__()\n","        self.num_hid = num_hid\n","        self.emb = tf.keras.layers.Embedding(num_vocab, num_hid)\n","        #self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=num_hid)\n","        '''\n","        self.pos_emb = tf.math.divide(\n","            self.positional_encoding(maxlen-1, num_hid),\n","            tf.math.sqrt(tf.cast(num_hid, tf.float32)))\n","        '''\n","        self.pos_emb = self.positional_encoding(maxlen-1, num_hid)\n","\n","    def call(self, x):\n","        maxlen = tf.shape(x)[-1]\n","        x = self.emb(x)\n","        x = tf.math.multiply(x, tf.math.sqrt(tf.cast(self.num_hid, tf.float32)))\n","        '''\n","        positions = tf.range(start=0, limit=maxlen, delta=1)\n","        positions = self.pos_emb(positions)\n","        return x + positions\n","        '''\n","        return x + self.pos_emb[:maxlen, :]\n","    \n","    def positional_encoding(self, maxlen, num_hid):\n","        depth = num_hid/2\n","        positions = tf.range(maxlen, dtype = tf.float32)[..., tf.newaxis]\n","        depths = tf.range(depth, dtype = tf.float32)[np.newaxis, :]/depth\n","        angle_rates = tf.math.divide(1, tf.math.pow(tf.cast(10000, tf.float32), depths))\n","        angle_rads = tf.linalg.matmul(positions, angle_rates)\n","        pos_encoding = tf.concat(\n","          [tf.math.sin(angle_rads), tf.math.cos(angle_rads)],\n","          axis=-1) \n","        return pos_encoding\n","\n","\n","class LandmarkEmbedding(layers.Layer):\n","    def __init__(self, num_hid=64, maxlen=100):\n","        super().__init__()\n","        self.conv1 = tf.keras.layers.Conv1D(\n","            num_hid, 11, padding=\"same\", activation=\"relu\"\n","        )\n","        self.conv2 = tf.keras.layers.Conv1D(\n","            num_hid, 11, padding=\"same\", activation=\"relu\"\n","        )\n","        self.conv3 = tf.keras.layers.Conv1D(\n","            num_hid, 11, padding=\"same\", activation=\"relu\"\n","        )\n","        self.pos_emb = self.positional_encoding(maxlen, num_hid)\n","        self.maxlen = maxlen\n","        self.num_hid = num_hid\n","\n","    def call(self, x):\n","        x = self.conv1(x)\n","        x = self.conv2(x)\n","        x = self.conv3(x)\n","        \n","        x = tf.math.multiply(x, tf.math.sqrt(tf.cast(self.num_hid, tf.float32)))\n","        x = x + self.pos_emb\n","        \n","        return x\n","    \n","    def positional_encoding(self, maxlen, num_hid):\n","        depth = num_hid/2\n","        positions = tf.range(maxlen, dtype = tf.float32)[..., tf.newaxis]\n","        depths = tf.range(depth, dtype = tf.float32)[np.newaxis, :]/depth\n","        angle_rates = tf.math.divide(1, tf.math.pow(tf.cast(10000, tf.float32), depths))\n","        angle_rads = tf.linalg.matmul(positions, angle_rates)\n","        pos_encoding = tf.concat(\n","          [tf.math.sin(angle_rads), tf.math.cos(angle_rads)],\n","          axis=-1) \n","        return pos_encoding"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-30T11:17:35.755734Z","iopub.status.busy":"2023-06-30T11:17:35.755190Z","iopub.status.idle":"2023-06-30T11:17:35.768697Z","shell.execute_reply":"2023-06-30T11:17:35.767550Z","shell.execute_reply.started":"2023-06-30T11:17:35.755697Z"},"trusted":true},"outputs":[],"source":["class TransformerEncoder(layers.Layer):\n","    def __init__(self, embed_dim, num_heads, feed_forward_dim, rate=0.1):\n","        super().__init__()\n","        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","        self.ffn = keras.Sequential(\n","            [\n","                layers.Dense(feed_forward_dim, activation=\"relu\"),\n","                layers.Dense(embed_dim),\n","            ]\n","        )\n","        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n","        self.dropout1 = layers.Dropout(rate)\n","        self.dropout2 = layers.Dropout(rate)\n","\n","    def call(self, inputs, training):\n","        attn_output = self.att(inputs, inputs)\n","        attn_output = self.dropout1(attn_output, training=training)\n","        out1 = self.layernorm1(inputs + attn_output)\n","        ffn_output = self.ffn(out1)\n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        return self.layernorm2(out1 + ffn_output)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-30T11:17:36.062847Z","iopub.status.busy":"2023-06-30T11:17:36.062470Z","iopub.status.idle":"2023-06-30T11:17:36.079608Z","shell.execute_reply":"2023-06-30T11:17:36.078526Z","shell.execute_reply.started":"2023-06-30T11:17:36.062815Z"},"trusted":true},"outputs":[],"source":["class TransformerDecoder(layers.Layer):\n","    def __init__(self, embed_dim, num_heads, feed_forward_dim, dropout_rate=0.1):\n","        super().__init__()\n","        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n","        self.self_att = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.enc_att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","        self.self_dropout = layers.Dropout(0.5)\n","        self.enc_dropout = layers.Dropout(0.1)\n","        self.ffn_dropout = layers.Dropout(0.1)\n","        self.ffn = keras.Sequential(\n","            [\n","                layers.Dense(feed_forward_dim, activation=\"relu\"),\n","                layers.Dense(embed_dim),\n","            ]\n","        )\n","\n","    def causal_attention_mask(self, batch_size, n_dest, n_src, dtype):\n","        \"\"\"Masks the upper half of the dot product matrix in self attention.\n","\n","        This prevents flow of information from future tokens to current token.\n","        1's in the lower triangle, counting from the lower right corner.\n","        \"\"\"\n","        i = tf.range(n_dest)[:, None]\n","        j = tf.range(n_src)\n","        m = i >= j - n_src + n_dest\n","        mask = tf.cast(m, dtype)\n","        mask = tf.reshape(mask, [1, n_dest, n_src])\n","        mult = tf.concat(\n","            [batch_size[..., tf.newaxis], tf.constant([1, 1], dtype=tf.int32)], 0\n","        )\n","        return tf.tile(mask, mult)\n","\n","    def call(self, enc_out, target, training):\n","        input_shape = tf.shape(target)\n","        batch_size = input_shape[0]\n","        seq_len = input_shape[1]\n","        causal_mask = self.causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n","        target_att = self.self_att(target, target, attention_mask=causal_mask)\n","        target_norm = self.layernorm1(target + self.self_dropout(target_att, training = training))\n","        enc_out = self.enc_att(target_norm, enc_out)\n","        enc_out_norm = self.layernorm2(self.enc_dropout(enc_out, training = training) + target_norm)\n","        ffn_out = self.ffn(enc_out_norm)\n","        ffn_out_norm = self.layernorm3(enc_out_norm + self.ffn_dropout(ffn_out, training = training))\n","        return ffn_out_norm"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-30T11:17:36.431050Z","iopub.status.busy":"2023-06-30T11:17:36.430701Z","iopub.status.idle":"2023-06-30T11:17:36.456211Z","shell.execute_reply":"2023-06-30T11:17:36.455081Z","shell.execute_reply.started":"2023-06-30T11:17:36.431023Z"},"trusted":true},"outputs":[],"source":["class Transformer(keras.Model):\n","    def __init__(\n","        self,\n","        num_hid=64,\n","        num_head=2,\n","        num_feed_forward=128,\n","        source_maxlen=100,\n","        target_maxlen=100,\n","        num_layers_enc=4,\n","        num_layers_dec=1,\n","        num_classes=62,\n","    ):\n","        super().__init__()\n","        self.loss_metric = keras.metrics.Mean(name=\"loss\")\n","        self.num_layers_enc = num_layers_enc\n","        self.num_layers_dec = num_layers_dec\n","        self.target_maxlen = target_maxlen\n","        self.num_classes = num_classes\n","\n","        self.enc_input = LandmarkEmbedding(num_hid=num_hid, maxlen=source_maxlen)\n","        self.dec_input = TokenEmbedding(\n","            num_vocab=num_classes, maxlen=target_maxlen, num_hid=num_hid\n","        )\n","\n","        self.encoder = keras.Sequential(\n","            [self.enc_input]\n","            + [\n","                TransformerEncoder(num_hid, num_head, num_feed_forward)\n","                for _ in range(num_layers_enc)\n","            ]\n","        )\n","\n","        for i in range(num_layers_dec):\n","            setattr(\n","                self,\n","                f\"dec_layer_{i}\",\n","                TransformerDecoder(num_hid, num_head, num_feed_forward),\n","            )\n","\n","        self.classifier = layers.Dense(num_classes)\n","\n","    def decode(self, enc_out, target, training):\n","        y = self.dec_input(target)\n","        for i in range(self.num_layers_dec):\n","            y = getattr(self, f\"dec_layer_{i}\")(enc_out, y, training)\n","        return y\n","\n","    def call(self, inputs, training):\n","        source = inputs[0]\n","        target = inputs[1]\n","        x = self.encoder(source, training)\n","        y = self.decode(x, target, training)\n","        return self.classifier(y)\n","\n","    @property\n","    def metrics(self):\n","        return [self.loss_metric]\n","\n","    def train_step(self, batch):\n","        \"\"\"Processes one batch inside model.fit().\"\"\"\n","        source = batch[0]\n","        target = batch[1]\n","\n","        input_shape = tf.shape(target)\n","        batch_size = input_shape[0]\n","        \n","        dec_input = target[:, :-1]\n","        dec_target = target[:, 1:]\n","        with tf.GradientTape() as tape:\n","            preds = self([source, dec_input])\n","            one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n","            mask = tf.math.logical_not(tf.math.equal(dec_target, CONFIG.mask_token_idx))\n","            loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n","        trainable_vars = self.trainable_variables\n","        gradients = tape.gradient(loss, trainable_vars)\n","        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n","        self.loss_metric.update_state(loss)\n","        return {\"loss\": self.loss_metric.result()}\n","\n","    def test_step(self, batch):        \n","        source = batch[0]\n","        target = batch[1]\n","\n","        input_shape = tf.shape(target)\n","        batch_size = input_shape[0]\n","        \n","        dec_input = target[:, :-1]\n","        dec_target = target[:, 1:]\n","        preds = self([source, dec_input])\n","        one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n","        mask = tf.math.logical_not(tf.math.equal(dec_target, CONFIG.mask_token_idx))\n","        loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n","        self.loss_metric.update_state(loss)\n","        return {\"loss\": self.loss_metric.result()}\n","\n","    def generate(self, source, target_start_token_idx):\n","        \"\"\"Performs inference over one batch of inputs using greedy decoding.\"\"\"\n","        bs = tf.shape(source)[0]\n","        enc = self.encoder(source, training = False)\n","        dec_input = tf.ones((bs, 1), dtype=tf.int32) * target_start_token_idx\n","        dec_logits = []\n","        for i in range(self.target_maxlen - 1):\n","            dec_out = self.decode(enc, dec_input, training = False)\n","            logits = self.classifier(dec_out)\n","            logits = tf.argmax(logits, axis=-1, output_type=tf.int32)\n","            last_logit = logits[:, -1][..., tf.newaxis]\n","            dec_logits.append(last_logit)\n","            dec_input = tf.concat([dec_input, last_logit], axis=-1)\n","        return dec_input"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-30T11:17:37.343490Z","iopub.status.busy":"2023-06-30T11:17:37.343086Z","iopub.status.idle":"2023-06-30T11:17:40.646519Z","shell.execute_reply":"2023-06-30T11:17:40.645551Z","shell.execute_reply.started":"2023-06-30T11:17:37.343457Z"},"trusted":true},"outputs":[],"source":["batch = next(iter(val_dataset))\n","idx_to_char = list(char_to_ord.keys())\n","\n","model = Transformer(\n","    num_hid=200,\n","    num_head=4,\n","    num_feed_forward=400,\n","    source_maxlen = FRAME_LEN,\n","    target_maxlen=64,\n","    num_layers_enc=2,\n","    num_layers_dec=1,\n","    num_classes=62,\n",")\n","\n","\n","loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1)\n","\n","\n","optimizer = keras.optimizers.Adam(0.0001)\n","model.compile(optimizer=optimizer, loss=loss_fn)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-30T11:17:40.648776Z","iopub.status.busy":"2023-06-30T11:17:40.648415Z"},"trusted":true},"outputs":[],"source":["%%time\n","history = model.fit(train_dataset, verbose = 2, validation_data=val_dataset, epochs=13)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(model.summary())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["batches = [batch for batch in val_dataset]\n","\n","preds_list = []\n","ground_truth_list = []\n","\n","for batch in batches[:1]:\n","    source = batch[0]\n","    target = batch[1].numpy()\n","    bs = tf.shape(source)[0]\n","    preds = model.generate(source, start_token_idx)\n","    preds = preds.numpy()\n","\n","    for i in range(bs):\n","        target_text = \"\".join([idx_to_char[_] for _ in target[i, :]])\n","        ground_truth_list.append(target_text.replace('P', ''))\n","        prediction = \"\"\n","        for idx in preds[i, :]:\n","            prediction += idx_to_char[idx]\n","            if idx == end_token_idx:\n","                break\n","        preds_list.append(prediction)\n","\n","for i in range(10):\n","    print(ground_truth_list[i])\n","    print(preds_list[i])\n","    print('\\n~~~\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ground_truth_processed = [ground_truth_list[i][1:-1] for i in range(len(ground_truth_list))]\n","preds_list_processed = [preds_list[i][1:-1] for i in range(len(preds_list))]\n","lev_dist = [lev.distance(ground_truth_processed[i], preds_list_processed[i]) \n","            for i in range(len(preds_list_processed))]\n","N = [len(phrase) for phrase in ground_truth_processed]\n","\n","print('Validation score: '+str((np.sum(N) - np.sum(lev_dist))/np.sum(N)))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class TFLiteModel(tf.Module):\n","    def __init__(self, model):\n","        super(TFLiteModel, self).__init__()\n","        self.target_start_token_idx = start_token_idx\n","        self.target_end_token_idx = end_token_idx\n","        # Load the feature generation and main models\n","        self.model = model\n","    \n","    @tf.function(input_signature=[tf.TensorSpec(shape=[None, len(SEL_COLS)], dtype=tf.float32, name='inputs')])\n","    def __call__(self, inputs, training=False):\n","        # Preprocess Data\n","        x = tf.cast(inputs, tf.float32)\n","        x = x[None]\n","        x = tf.cond(tf.shape(x)[1] == 0, lambda: tf.zeros((1, 1, len(SEL_COLS))), lambda: tf.identity(x))\n","        x = x[0]\n","        x = pre_process(x)\n","        x = x[None]\n","        x = self.model.generate(x, self.target_start_token_idx)\n","        x = x[0]\n","        idx = tf.argmax(tf.cast(tf.equal(x, self.target_end_token_idx), tf.int32))\n","        idx = tf.where(tf.math.less(idx, 1), tf.constant(2, dtype=tf.int64), idx)\n","        x = x[1:idx]\n","        x = tf.one_hot(x, 59)\n","        return {'outputs': x}\n","    \n","tflitemodel_base = TFLiteModel(model)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.save_weights(\"model.h5\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["keras_model_converter = tf.lite.TFLiteConverter.from_keras_model(tflitemodel_base)\n","keras_model_converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]#, tf.lite.OpsSet.SELECT_TF_OPS]\n","tflite_model = keras_model_converter.convert()\n","with open('/kaggle/working/model.tflite', 'wb') as f:\n","    f.write(tflite_model)\n","    \n","infargs = {\"selected_columns\" : SEL_COLS}\n","\n","with open('inference_args.json', \"w\") as json_file:\n","    json.dump(infargs, json_file)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!zip submission.zip  './model.tflite' './inference_args.json'"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["interpreter = tf.lite.Interpreter(\"model.tflite\")\n","\n","REQUIRED_SIGNATURE = \"serving_default\"\n","REQUIRED_OUTPUT = \"outputs\"\n","\n","with open (\"/kaggle/input/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\n","    character_map = json.load(f)\n","rev_character_map = {j:i for i,j in character_map.items()}\n","\n","found_signatures = list(interpreter.get_signature_list().keys())\n","\n","if REQUIRED_SIGNATURE not in found_signatures:\n","    raise KernelEvalException('Required input signature not found.')\n","\n","prediction_fn = interpreter.get_signature_runner(\"serving_default\")\n","output = prediction_fn(inputs=batch[0][0])\n","prediction_str = \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(output[REQUIRED_OUTPUT], axis=1)])\n","print(prediction_str)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
