{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","editable":false,"execution":{"iopub.execute_input":"2023-06-29T07:39:02.760032Z","iopub.status.busy":"2023-06-29T07:39:02.759446Z","iopub.status.idle":"2023-06-29T07:39:11.851606Z","shell.execute_reply":"2023-06-29T07:39:11.850151Z","shell.execute_reply.started":"2023-06-29T07:39:02.760000Z"},"trusted":true},"outputs":[],"source":["from tensorflow.keras import layers\n","from tensorflow import keras\n","import tensorflow as tf\n","from tqdm import tqdm\n","import pandas as pd\n","import numpy as np\n","import json\n","import os\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-06-29T07:39:57.727322Z","iopub.status.busy":"2023-06-29T07:39:57.726677Z","iopub.status.idle":"2023-06-29T07:39:57.732347Z","shell.execute_reply":"2023-06-29T07:39:57.731420Z","shell.execute_reply.started":"2023-06-29T07:39:57.727293Z"},"trusted":true},"outputs":[],"source":["class CONFIG:\n","    ROOT_DIR = '/kaggle/input/asl-fingerspelling'\n","    TRAIN_DATA = '/kaggle/input/asl-fingerspelling/train.csv'\n","    TRAIN_DIR = '/kaggle/input/asl-fingerspelling/train_landmarks'\n","    CHAR_PREDICTION_INDEX_MAP = '/kaggle/input/asl-fingerspelling/character_to_prediction_index.json'\n","    SUP_METADATA = '/kaggle/input/asl-fingerspelling/supplemental_metadata.csv'\n","    SUP_LANDMARK_DIR = '/kaggle/input/asl-fingerspelling/supplemental_landmarks'\n","    \n","    OUTPUT_PREPROCESSING = 'tfds'"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-06-29T07:39:59.041078Z","iopub.status.busy":"2023-06-29T07:39:59.040685Z","iopub.status.idle":"2023-06-29T07:39:59.048536Z","shell.execute_reply":"2023-06-29T07:39:59.047361Z","shell.execute_reply.started":"2023-06-29T07:39:59.041052Z"},"trusted":true},"outputs":[],"source":["with open (CONFIG.CHAR_PREDICTION_INDEX_MAP, \"r\") as f:\n","    char_to_ord = json.load(f)\n","\n","ord_to_char = {j:i for i, j in char_to_ord.items()}"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-06-29T07:39:59.863838Z","iopub.status.busy":"2023-06-29T07:39:59.863381Z","iopub.status.idle":"2023-06-29T07:39:59.873855Z","shell.execute_reply":"2023-06-29T07:39:59.872823Z","shell.execute_reply.started":"2023-06-29T07:39:59.863809Z"},"trusted":true},"outputs":[],"source":["# Memory saving function credit to https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n","def reduce_mem_usage(df):\n","    \"\"\" iterate through all the columns of a dataframe and modify the data type\n","        to reduce memory usage.        \n","    \"\"\"\n","\n","    for col in df.columns:\n","        col_type = df[col].dtype\n","\n","        if col_type != object:\n","            c_min = df[col].min()\n","            c_max = df[col].max()\n","            if str(col_type)[:3] == 'int':\n","                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n","                    df[col] = df[col].astype(np.int8)\n","                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n","                    df[col] = df[col].astype(np.int16)\n","                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n","                    df[col] = df[col].astype(np.int32)\n","                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n","                    df[col] = df[col].astype(np.int64)  \n","            else:\n","                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n","                    df[col] = df[col].astype(np.float16)\n","                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n","                    df[col] = df[col].astype(np.float32)\n","                else:\n","                    df[col] = df[col].astype(np.float64)\n","\n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-06-29T07:40:00.247656Z","iopub.status.busy":"2023-06-29T07:40:00.247330Z","iopub.status.idle":"2023-06-29T07:40:00.426433Z","shell.execute_reply":"2023-06-29T07:40:00.425483Z","shell.execute_reply.started":"2023-06-29T07:40:00.247630Z"},"trusted":true},"outputs":[],"source":["train_df = pd.read_csv(CONFIG.TRAIN_DATA)\n","train_df[\"phrase_bytes\"] = train_df[\"phrase\"].map(lambda x: x.encode(\"utf-8\"))\n","train_df = reduce_mem_usage(train_df)\n","train_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-06-29T07:40:02.739040Z","iopub.status.busy":"2023-06-29T07:40:02.738654Z","iopub.status.idle":"2023-06-29T07:40:02.747230Z","shell.execute_reply":"2023-06-29T07:40:02.746121Z","shell.execute_reply.started":"2023-06-29T07:40:02.738993Z"},"trusted":true},"outputs":[],"source":["# Can be added more landmark here\n","LPOSE = [13, 15, 17, 19, 21]\n","RPOSE = [14, 16, 18, 20, 22]\n","POSE = LPOSE + RPOSE\n","# POSE = list(range(0, 32))\n","\n","FACE = [0, 9, 11, 13, 14, 17, 117, 118, 119, 199, 346, 347, 348]\n","\n","RHAND_LBLS = [f'x_right_hand_{i}' for i in range(21)] + [f'y_right_hand_{i}' for i in range(21)] + [f'z_right_hand_{i}' for i in range(21)]\n","LHAND_LBLS = [ f'x_left_hand_{i}' for i in range(21)] + [ f'y_left_hand_{i}' for i in range(21)] + [ f'z_left_hand_{i}' for i in range(21)]\n","POSE_LBLS = [f'x_pose_{i}' for i in POSE] + [f'y_pose_{i}' for i in POSE] + [f'z_pose_{i}' for i in POSE]\n","FACE_LBLS = [f'x_face_{i}' for i in FACE] + [f'y_face_{i}' for i in FACE] + [f'z_face_{i}' for i in FACE]"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-06-29T07:40:03.412202Z","iopub.status.busy":"2023-06-29T07:40:03.411832Z","iopub.status.idle":"2023-06-29T07:40:03.419131Z","shell.execute_reply":"2023-06-29T07:40:03.418018Z","shell.execute_reply.started":"2023-06-29T07:40:03.412175Z"},"trusted":true},"outputs":[],"source":["X = [f'x_right_hand_{i}' for i in range(21)] + [f'x_left_hand_{i}' for i in range(21)] + [f'x_pose_{i}' for i in POSE] + [f'x_face_{i}' for i in FACE]\n","Y = [f'y_right_hand_{i}' for i in range(21)] + [f'y_left_hand_{i}' for i in range(21)] + [f'y_pose_{i}' for i in POSE] + [f'y_face_{i}' for i in FACE]\n","Z = [f'z_right_hand_{i}' for i in range(21)] + [f'z_left_hand_{i}' for i in range(21)] + [f'z_pose_{i}' for i in POSE] + [f'z_face_{i}' for i in FACE]"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-06-29T07:40:03.744188Z","iopub.status.busy":"2023-06-29T07:40:03.743861Z","iopub.status.idle":"2023-06-29T07:40:03.750869Z","shell.execute_reply":"2023-06-29T07:40:03.749941Z","shell.execute_reply.started":"2023-06-29T07:40:03.744161Z"},"trusted":true},"outputs":[],"source":["SEL_COLS = X + Y + Z\n","FRAME_LEN = 128\n","\n","X_IDX = [i for i, col in enumerate(SEL_COLS)  if \"x_\" in col]\n","Y_IDX = [i for i, col in enumerate(SEL_COLS)  if \"y_\" in col]\n","Z_IDX = [i for i, col in enumerate(SEL_COLS)  if \"z_\" in col]\n","\n","RHAND_IDX = [i for i, col in enumerate(SEL_COLS)  if \"right_hand\" in col]\n","LHAND_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"left_hand\" in col]\n","RPOSE_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in RPOSE]\n","LPOSE_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"pose\" in col and int(col[-2:]) in LPOSE]\n","FACE_IDX = [i for i, col in enumerate(SEL_COLS)  if  \"face\" in col]"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-06-29T07:40:04.412324Z","iopub.status.busy":"2023-06-29T07:40:04.411984Z","iopub.status.idle":"2023-06-29T07:40:04.435154Z","shell.execute_reply":"2023-06-29T07:40:04.433282Z","shell.execute_reply.started":"2023-06-29T07:40:04.412298Z"},"trusted":true},"outputs":[],"source":["# Preprocessing the landmarks\n","def resize_pad(x):\n","    if tf.shape(x)[0] < FRAME_LEN:\n","        x = tf.pad(x, ([[0, FRAME_LEN-tf.shape(x)[0]], [0, 0], [0, 0]]))\n","    else:\n","        x = tf.image.resize(x, (FRAME_LEN, tf.shape(x)[1]))\n","    return x\n","\n","\n","def preprocess_landmark(x):\n","    rhand = tf.gather(x, RHAND_IDX, axis=1)\n","    lhand = tf.gather(x, LHAND_IDX, axis=1)\n","    rpose = tf.gather(x, RPOSE_IDX, axis=1)\n","    lpose = tf.gather(x, LPOSE_IDX, axis=1)\n","    face  = tf.gather(x, FACE_IDX, axis=1)\n","    \n","    rnan_idx = tf.reduce_any(tf.math.is_nan(rhand), axis=1)\n","    lnan_idx = tf.reduce_any(tf.math.is_nan(lhand), axis=1)\n","    \n","    rnans = tf.math.count_nonzero(rnan_idx)\n","    lnans = tf.math.count_nonzero(lnan_idx)\n","    \n","    # For dominant hand\n","    if rnans > lnans:\n","        hand = lhand\n","        pose = lpose\n","        face = face\n","        \n","        hand_x = hand[:, 0*(len(LHAND_IDX)//3) : 1*(len(LHAND_IDX)//3)]\n","        hand_y = hand[:, 1*(len(LHAND_IDX)//3) : 2*(len(LHAND_IDX)//3)]\n","        hand_z = hand[:, 2*(len(LHAND_IDX)//3) : 3*(len(LHAND_IDX)//3)]\n","        hand = tf.concat([1-hand_x, hand_y, hand_z], axis=1)\n","        \n","        pose_x = pose[:, 0*(len(LPOSE_IDX)//3) : 1*(len(LPOSE_IDX)//3)]\n","        pose_y = pose[:, 1*(len(LPOSE_IDX)//3) : 2*(len(LPOSE_IDX)//3)]\n","        pose_z = pose[:, 2*(len(LPOSE_IDX)//3) : 3*(len(LPOSE_IDX)//3)]\n","        pose = tf.concat([1-pose_x, pose_y, pose_z], axis=1)\n","    else:\n","        hand = rhand\n","        pose = rpose\n","        face = face\n","    \n","    hand_x = hand[:, 0*(len(LHAND_IDX)//3) : 1*(len(LHAND_IDX)//3)]\n","    hand_y = hand[:, 1*(len(LHAND_IDX)//3) : 2*(len(LHAND_IDX)//3)]\n","    hand_z = hand[:, 2*(len(LHAND_IDX)//3) : 3*(len(LHAND_IDX)//3)]\n","    hand = tf.concat([hand_x[..., tf.newaxis], hand_y[..., tf.newaxis], hand_z[..., tf.newaxis]], axis=-1)\n","    \n","    mean = tf.math.reduce_mean(hand, axis=1)[:, tf.newaxis, :]\n","    std = tf.math.reduce_std(hand, axis=1)[:, tf.newaxis, :]\n","    hand = (hand - mean) / std\n","\n","    pose_x = pose[:, 0*(len(LPOSE_IDX)//3) : 1*(len(LPOSE_IDX)//3)]\n","    pose_y = pose[:, 1*(len(LPOSE_IDX)//3) : 2*(len(LPOSE_IDX)//3)]\n","    pose_z = pose[:, 2*(len(LPOSE_IDX)//3) : 3*(len(LPOSE_IDX)//3)]\n","    pose = tf.concat([pose_x[..., tf.newaxis], pose_y[..., tf.newaxis], pose_z[..., tf.newaxis]], axis=-1)\n","    \n","    mean = tf.math.reduce_mean(pose, axis=1)[:, tf.newaxis, :]\n","    std = tf.math.reduce_std(pose, axis=1)[:, tf.newaxis, :]\n","    pose = (pose - mean) / std\n","    \n","    face_x = face[:, 0*(len(FACE_IDX)//3) : 1*(len(FACE_IDX)//3)]\n","    face_y = face[:, 1*(len(FACE_IDX)//3) : 2*(len(FACE_IDX)//3)]\n","    face_z = face[:, 2*(len(FACE_IDX)//3) : 3*(len(FACE_IDX)//3)]\n","    face = tf.concat([face_x[..., tf.newaxis], face_y[..., tf.newaxis], face_z[..., tf.newaxis]], axis=-1)\n","    \n","    mean = tf.math.reduce_mean(face, axis=1)[:, tf.newaxis, :]\n","    std = tf.math.reduce_std(face, axis=1)[:, tf.newaxis, :]\n","    face = (face - mean) / std\n","    \n","    x = tf.concat([hand, pose, face], axis=1)\n","    x = resize_pad(x)\n","    \n","    x = tf.where(tf.math.is_nan(x), tf.zeros_like(x), x)\n","    x = tf.reshape(x, (FRAME_LEN, len(LHAND_IDX) + len(LPOSE_IDX) + len(FACE_IDX)))\n","    return x"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-06-29T07:40:05.326304Z","iopub.status.busy":"2023-06-29T07:40:05.325926Z","iopub.status.idle":"2023-06-29T07:40:05.332239Z","shell.execute_reply":"2023-06-29T07:40:05.330871Z","shell.execute_reply.started":"2023-06-29T07:40:05.326276Z"},"trusted":true},"outputs":[],"source":["def load_relevant_data_subset(pq_path):\n","    return pd.read_parquet(pq_path, columns=SEL_COLS)"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-06-29T07:40:05.694988Z","iopub.status.busy":"2023-06-29T07:40:05.694551Z","iopub.status.idle":"2023-06-29T07:40:08.254432Z","shell.execute_reply":"2023-06-29T07:40:08.253465Z","shell.execute_reply.started":"2023-06-29T07:40:05.694956Z"},"trusted":true},"outputs":[],"source":["file_id = train_df.file_id.iloc[0]\n","pqfile = f\"{CONFIG.ROOT_DIR}/train_landmarks/{file_id}.parquet\"\n","seq_refs = train_df.loc[train_df.file_id == file_id]\n","seqs = load_relevant_data_subset(pqfile)\n","\n","seq_id = seq_refs.sequence_id.iloc[0]\n","frames = seqs.iloc[seqs.index == seq_id]\n","phrase = str(train_df.loc[train_df.sequence_id == seq_id].phrase.iloc[0])\n","\n","print(preprocess_landmark(frames).shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-06-29T07:40:09.958723Z","iopub.status.busy":"2023-06-29T07:40:09.957641Z","iopub.status.idle":"2023-06-29T07:40:09.976291Z","shell.execute_reply":"2023-06-29T07:40:09.975177Z","shell.execute_reply.started":"2023-06-29T07:40:09.958676Z"},"trusted":true},"outputs":[],"source":["table = tf.lookup.StaticHashTable(\n","    initializer=tf.lookup.KeyValueTensorInitializer(\n","        keys=list(char_to_ord.keys()),\n","        values=list(char_to_ord.values()),\n","    ),\n","    default_value=tf.constant(-1),\n","    name=\"class_weight\"\n",")\n","\n","mask_idx = char_to_ord['#']\n","\n","def preprocess_fn(landmarks, phrase):\n","    phrase = ';' + phrase + '['\n","    phrase = tf.strings.bytes_split(phrase)\n","    phrase = table.lookup(phrase)\n","    phrase = tf.pad(phrase, paddings=[[0, 64 - tf.shape(phrase)[0]]], constant_values=mask_idx)\n","    return preprocess_landmark(landmarks), phrase\n","\n","def decode_fn(record_bytes):\n","    schema = {COL: tf.io.VarLenFeature(dtype=tf.float32) for COL in SEL_COLS}\n","    schema[\"phrase\"] = tf.io.FixedLenFeature([], dtype=tf.string)\n","    features = tf.io.parse_single_example(record_bytes, schema)\n","    phrase = features[\"phrase\"]\n","    landmarks = ([tf.sparse.to_dense(features[COL]) for COL in SEL_COLS])\n","    landmarks = tf.transpose(landmarks)\n","    return landmarks, phrase"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-06-29T07:40:51.427964Z","iopub.status.busy":"2023-06-29T07:40:51.427541Z","iopub.status.idle":"2023-06-29T08:01:09.430615Z","shell.execute_reply":"2023-06-29T08:01:09.429169Z","shell.execute_reply.started":"2023-06-29T07:40:51.427934Z"},"trusted":true},"outputs":[],"source":["from skimage.transform import resize\n","\n","for file_id in tqdm(train_df.file_id.unique()):\n","    pqfile = f\"{CONFIG.ROOT_DIR}/train_landmarks/{file_id}.parquet\"\n","    if not os.path.isdir(\"tfds\"): os.mkdir(\"tfds\")\n","    tffile = f\"tfds/{file_id}.tfrecord\"\n","    seq_refs = train_df.loc[train_df.file_id == file_id]\n","    seqs = load_relevant_data_subset(pqfile)\n","    \n","    with tf.io.TFRecordWriter(tffile) as file_writer:\n","        for seq_id, phrase in zip(seq_refs.sequence_id, seq_refs.phrase_bytes):\n","            frames = seqs.iloc[seqs.index == seq_id]\n","            frames128 = frames.fillna(-10).to_numpy()\n","            frames128 = resize(frames128, (FRAME_LEN, len(SEL_COLS)))\n","            frames = pd.DataFrame(data = frames128, columns=frames.columns)\n","            \n","            features = {COL: tf.train.Feature(float_list=tf.train.FloatList(value=frames[COL])) for COL in SEL_COLS}\n","            features[\"phrase\"] = tf.train.Feature(bytes_list=tf.train.BytesList(value=[phrase]))\n","            record_bytes = tf.train.Example(features=tf.train.Features(feature=features)).SerializeToString()\n","            file_writer.write(record_bytes)"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-06-29T08:01:49.839699Z","iopub.status.busy":"2023-06-29T08:01:49.838995Z","iopub.status.idle":"2023-06-29T08:05:24.057646Z","shell.execute_reply":"2023-06-29T08:05:24.056288Z","shell.execute_reply.started":"2023-06-29T08:01:49.839616Z"},"trusted":true},"outputs":[],"source":["!tar -zcvf asl.tar.gz /kaggle/working/tfds"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2023-06-29T08:18:10.046113Z","iopub.status.busy":"2023-06-29T08:18:10.045751Z","iopub.status.idle":"2023-06-29T08:18:12.597463Z","shell.execute_reply":"2023-06-29T08:18:12.596123Z","shell.execute_reply.started":"2023-06-29T08:18:10.046087Z"},"trusted":true},"outputs":[],"source":["inpdir = \"/kaggle/working/tfds\"\n","tffiles = train_df.file_id.map(lambda x: f'{CONFIG.ROOT_DIR}/train/{x}.tfrecord').unique()\n","\n","batch_size = 32\n","val_len = int(0.1 * len(tffiles))\n","\n","train_dataset = tf.data.TFRecordDataset(tffiles[val_len:]).map(decode_fn).map(preprocess_fn).shuffle(buffer_size=500).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n","val_dataset = tf.data.TFRecordDataset(tffiles[:val_len]).map(decode_fn).map(preprocess_fn).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n","test_dataset = tf.data.TFRecordDataset(tffiles).map(decode_fn).prefetch(buffer_size=tf.data.AUTOTUNE)"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
